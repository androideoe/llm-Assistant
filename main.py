from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain_community.llms import Ollama
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
import logging
from sse_starlette.sse import ServerSentEvent, EventSourceResponse
import asyncio
from typing import Optional
from langchain.memory import ConversationBufferMemory
import uuid
from fastapi import Query
from typing import Dict
from langchain.chains import ConversationChain
# RAGç›¸å…³å¯¼å…¥
from langchain_community.document_loaders import TextLoader
from langchain_community.document_loaders.pdf import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings
from langchain.chains import RetrievalQA
import os
import time
from pathlib import Path
import functools
from typing import Tuple, List, Optional
import time

# æ·»åŠ ç¼“å­˜æœºåˆ¶
# ç¼“å­˜è£…é¥°å™¨
def cache_result(ttl_seconds: int = 300):
    """ç®€å•çš„å†…å­˜ç¼“å­˜è£…é¥°å™¨"""
    cache = {}
    
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # åˆ›å»ºç¼“å­˜é”®
            cache_key = str(args) + str(sorted(kwargs.items()))
            
            # æ£€æŸ¥ç¼“å­˜
            if cache_key in cache:
                result, timestamp = cache[cache_key]
                if time.time() - timestamp < ttl_seconds:
                    return result
            
            # æ‰§è¡Œå‡½æ•°
            result = func(*args, **kwargs)
            
            # ç¼“å­˜ç»“æœ
            cache[cache_key] = (result, time.time())
            return result
        return wrapper
    return decorator

# æ·»åŠ æ€§èƒ½ç›‘æ§
import time
from contextlib import contextmanager

@contextmanager
def performance_monitor(operation_name: str):
    """æ€§èƒ½ç›‘æ§ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    start_time = time.time()
    try:
        yield
    finally:
        elapsed = time.time() - start_time
        logger.info(f"æ€§èƒ½ç›‘æ§ - {operation_name}: {elapsed:.3f}ç§’")

# ä¼˜åŒ–åçš„ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°
def calculate_similarity_score_optimized(distance_score: float, method: str = "sigmoid") -> float:
    """
    ä¼˜åŒ–åçš„ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°ï¼Œå‡å°‘å¤æ‚è®¡ç®—
    """
    import math
    
    # å¤„ç†å¼‚å¸¸å€¼
    if distance_score < 0:
        distance_score = 0
    
    if method == "cosine":
        # å¯¹äºå½’ä¸€åŒ–å‘é‡ï¼Œç›´æ¥ä½¿ç”¨
        return max(0, min(1, distance_score))
    elif method == "sigmoid":
        # ç®€åŒ–çš„sigmoidè®¡ç®—
        try:
            if distance_score > 1000:
                return 1 / (1 + math.log(1 + distance_score / 1000))
            elif distance_score > 100:
                scaled_distance = distance_score / 100
                exp_term = min(scaled_distance - 2, 700)
                exp_term = max(exp_term, -700)
                return 1 / (1 + math.exp(exp_term))
            else:
                # ç®€åŒ–çš„sigmoidè®¡ç®—
                exp_term = min(distance_score - 10, 700)
                exp_term = max(exp_term, -700)
                similarity = 1 / (1 + math.exp(exp_term))
                return min(similarity, 0.95)
        except (OverflowError, ValueError):
            return max(0, 1 - distance_score / 10000.0)
    else:
        # é»˜è®¤çº¿æ€§è½¬æ¢
        normalized_distance = min(distance_score / 1000.0, 1.0)
        return max(0, 1 - normalized_distance)

# ä¼˜åŒ–åçš„æ£€ç´¢å‡½æ•°ï¼ˆå¸¦æ€§èƒ½ç›‘æ§ï¼‰
@cache_result(ttl_seconds=60)  # ç¼“å­˜1åˆ†é’Ÿ
def optimized_retrieval(vector_store, query: str, max_docs: int = 5, 
                       relevance_threshold: float = 0.5) -> Tuple[List, float, Dict]:
    """
    ä¼˜åŒ–åçš„æ£€ç´¢å‡½æ•°ï¼Œå‡å°‘ä¸å¿…è¦çš„è®¡ç®—å’Œæ—¥å¿—
    """
    retrieval_start_time = time.time()
    logger.info(f"ğŸ” RAGæ£€ç´¢å¼€å§‹ - æŸ¥è¯¢: '{query[:50]}...' | æœ€å¤§æ–‡æ¡£æ•°: {max_docs} | ç›¸å…³æ€§é˜ˆå€¼: {relevance_threshold}")
    
    with performance_monitor("å‘é‡æ£€ç´¢"):
        try:
            # ç›´æ¥æ£€ç´¢æ‰€éœ€æ•°é‡çš„æ–‡æ¡£
            initial_k = min(max_docs * 2, 10)
            logger.debug(f"ğŸ“Š åˆå§‹æ£€ç´¢å‚æ•° - Kå€¼: {initial_k}")
            
            # æ‰§è¡Œæ£€ç´¢
            search_start_time = time.time()
            docs = vector_store.similarity_search_with_score(query, k=initial_k)
            search_duration = time.time() - search_start_time
            logger.info(f"âš¡ å‘é‡æ£€ç´¢å®Œæˆ - è€—æ—¶: {search_duration:.3f}ç§’ | åŸå§‹ç»“æœæ•°: {len(docs)}")
            
            if not docs:
                logger.warning("âŒ RAGæ£€ç´¢å¤±è´¥ - æœªæ‰¾åˆ°ä»»ä½•ç›¸å…³æ–‡æ¡£")
                return [], 0.0, {"retrieved": 0, "relevant": 0, "threshold": relevance_threshold}
            
            # å¿«é€Ÿå¤„ç†ç»“æœ
            doc_scores = []
            distances = []
            
            logger.debug("ğŸ“ˆ å¼€å§‹å¤„ç†æ£€ç´¢ç»“æœå’Œè®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°...")
            for i, result in enumerate(docs):
                try:
                    if isinstance(result, tuple) and len(result) == 2:
                        doc, score = result
                        score_float = float(score)
                        distances.append(score_float)
                        logger.debug(f"  æ–‡æ¡£ {i+1}: è·ç¦»åˆ†æ•° = {score_float:.4f}")
                    else:
                        logger.warning(f"âš ï¸  æ–‡æ¡£ {i+1}: æ ¼å¼å¼‚å¸¸ï¼Œè·³è¿‡")
                        continue
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†æ–‡æ¡£ {i+1} æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            if not distances:
                logger.error("âŒ RAGæ£€ç´¢å¤±è´¥ - æ— æœ‰æ•ˆè·ç¦»åˆ†æ•°")
                return [], 0.0, {"retrieved": len(docs), "relevant": 0, "threshold": relevance_threshold}
            
            # è·ç¦»ç»Ÿè®¡ä¿¡æ¯
            min_distance = min(distances)
            max_distance = max(distances)
            avg_distance = sum(distances) / len(distances)
            logger.info(f"ğŸ“Š è·ç¦»åˆ†æ•°ç»Ÿè®¡ - æœ€å°: {min_distance:.4f} | æœ€å¤§: {max_distance:.4f} | å¹³å‡: {avg_distance:.4f}")
            
            # å¿«é€Ÿæ£€æµ‹å‘é‡ç±»å‹
            is_normalized = max(distances) <= 1.0 and min(distances) >= 0.0
            similarity_method = "cosine" if is_normalized else "sigmoid"
            logger.debug(f"ğŸ”§ ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•: {similarity_method} (å½’ä¸€åŒ–: {is_normalized})")
            
            # æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦
            similarity_start_time = time.time()
            for i, (doc, score_float) in enumerate(zip([result[0] for result in docs if isinstance(result, tuple) and len(result) == 2], distances)):
                try:
                    similarity_score = calculate_similarity_score_optimized(score_float, method=similarity_method)
                    doc_scores.append((doc, similarity_score))
                    logger.debug(f"  æ–‡æ¡£ {i+1}: è·ç¦» {score_float:.4f} -> ç›¸ä¼¼åº¦ {similarity_score:.4f}")
                except Exception as e:
                    logger.error(f"âŒ è®¡ç®—æ–‡æ¡£ {i+1} ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            similarity_duration = time.time() - similarity_start_time
            logger.debug(f"âš¡ ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ - è€—æ—¶: {similarity_duration:.3f}ç§’")
            
            # æ’åºå¹¶è¿‡æ»¤
            doc_scores.sort(key=lambda x: x[1], reverse=True)
            
            max_similarity = doc_scores[0][1] if doc_scores else 0.0
            logger.info(f"ğŸ† æœ€é«˜ç›¸ä¼¼åº¦åˆ†æ•°: {max_similarity:.4f}")
            
            # è¿‡æ»¤ç›¸å…³æ–‡æ¡£
            relevant_docs = []
            filtered_scores = []
            for i, (doc, similarity_score) in enumerate(doc_scores):
                if similarity_score >= relevance_threshold:
                    relevant_docs.append(doc)
                    filtered_scores.append(similarity_score)
                    
                    # è®°å½•æ–‡æ¡£å†…å®¹é¢„è§ˆ
                    content_preview = doc.page_content[:100].replace('\n', ' ')
                    logger.info(f"âœ… ç›¸å…³æ–‡æ¡£ {len(relevant_docs)}: ç›¸ä¼¼åº¦ {similarity_score:.4f} | å†…å®¹: '{content_preview}...'")
                    
                    if len(relevant_docs) >= max_docs:
                        logger.info(f"ğŸ“‹ è¾¾åˆ°æœ€å¤§æ–‡æ¡£æ•°é™åˆ¶ ({max_docs})")
                        break
                else:
                    logger.debug(f"âŒ æ–‡æ¡£ {i+1}: ç›¸ä¼¼åº¦ {similarity_score:.4f} < é˜ˆå€¼ {relevance_threshold}ï¼Œå·²è¿‡æ»¤")
            
            retrieval_info = {
                "retrieved": len(docs),
                "relevant": len(relevant_docs),
                "threshold": relevance_threshold,
                "max_similarity": max_similarity,
                "similarity_method": similarity_method,
                "distance_stats": {
                    "min": min_distance,
                    "max": max_distance,
                    "avg": avg_distance
                },
                "is_normalized": is_normalized,
                "relevant_scores": filtered_scores,
                "search_duration": search_duration,
                "similarity_duration": similarity_duration
            }
            
            total_duration = time.time() - retrieval_start_time
            
            if relevant_docs:
                logger.info(f"ğŸ¯ RAGæ£€ç´¢æˆåŠŸ - æ€»è€—æ—¶: {total_duration:.3f}ç§’ | æ£€ç´¢åˆ°: {len(docs)}ä¸ª | ç›¸å…³: {len(relevant_docs)}ä¸ª | å¹³å‡ç›¸ä¼¼åº¦: {sum(filtered_scores)/len(filtered_scores):.4f}")
            else:
                logger.warning(f"âš ï¸  RAGæ£€ç´¢å®Œæˆä½†æ— ç›¸å…³æ–‡æ¡£ - æ€»è€—æ—¶: {total_duration:.3f}ç§’ | æ£€ç´¢åˆ°: {len(docs)}ä¸ª | æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.4f} < é˜ˆå€¼: {relevance_threshold}")
            
            return relevant_docs, max_similarity, retrieval_info
            
        except Exception as e:
            total_duration = time.time() - retrieval_start_time
            logger.error(f"âŒ RAGæ£€ç´¢å¼‚å¸¸ - æ€»è€—æ—¶: {total_duration:.3f}ç§’ | é”™è¯¯: {str(e)}", exc_info=True)
            return [], 0.0, {"retrieved": 0, "relevant": 0, "threshold": relevance_threshold, "error": str(e)}

# æ›¿æ¢åŸæœ‰çš„adaptive_retrievalå‡½æ•°
def adaptive_retrieval(vector_store, query: str, max_docs: int = 5, min_docs: int = 1, 
                       relevance_threshold: float = 0.5) -> tuple:
    """
    ä½¿ç”¨ä¼˜åŒ–åçš„æ£€ç´¢å‡½æ•°
    """
    return optimized_retrieval(vector_store, query, max_docs, relevance_threshold)

# è®¾ç½®å·¥ä½œç›®å½•ä¸ºå½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
WORKING_DIR = Path(__file__).parent
os.chdir(WORKING_DIR)

# é…ç½®æ—¥å¿— - æ›´è¯¦ç»†çš„æ—¥å¿—è®¾ç½®
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("api.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(title="å¤šæ¨¡å‹èŠå¤© API", description="ä¸æœ¬åœ°éƒ¨ç½²çš„Ollamaæ¨¡å‹äº¤äº’çš„APIæ¥å£ï¼Œé»˜è®¤ä½¿ç”¨Qwen3 1.7Bï¼ˆæ”¯æŒæ€è€ƒè¿‡ç¨‹ï¼‰ï¼Œå¯åˆ‡æ¢è‡³Llama 3.2", version="1.0")

# é…ç½®CORS - åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åº”æŒ‡å®šå…·ä½“çš„å‰ç«¯åŸŸå
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–Ollama LLM - æ›´å¥å£®çš„åˆå§‹åŒ–ï¼Œæ”¯æŒå¤šæ¨¡å‹
llm: Optional[Ollama] = None
current_model: str = "qwen3:1.7b"  # é»˜è®¤æ¨¡å‹æ”¹ä¸ºQwen3
available_models = ["llama3.2", "qwen3:1.7b"]  # å¯ç”¨æ¨¡å‹åˆ—è¡¨

# ä¼šè¯å†…å­˜æ± ï¼šsession_id -> ConversationBufferMemory
session_memories: Dict[str, ConversationBufferMemory] = {}

# RAGç›¸å…³å…¨å±€å˜é‡
embeddings: Optional[OllamaEmbeddings] = None
vector_store: Optional[FAISS] = None
document_sessions: Dict[str, FAISS] = {}  # session_id -> FAISS vector store

# çŸ¥è¯†åº“ç›®å½•é…ç½® - ä½¿ç”¨ç›¸å¯¹è·¯å¾„
KNOWLEDGE_BASE_DIR = Path("knowledge_base")
VECTOR_STORES_DIR = Path("vector_stores")

# ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
KNOWLEDGE_BASE_DIR.mkdir(exist_ok=True)
VECTOR_STORES_DIR.mkdir(exist_ok=True)

logger.info(f"å·¥ä½œç›®å½•è®¾ç½®ä¸º: {WORKING_DIR}")
logger.info(f"çŸ¥è¯†åº“ç›®å½•: {KNOWLEDGE_BASE_DIR.absolute()}")
logger.info(f"å‘é‡å­˜å‚¨ç›®å½•: {VECTOR_STORES_DIR.absolute()}")

# çŸ¥è¯†åº“æ–‡ä»¶ç±»å‹
SUPPORTED_EXTENSIONS = {'.txt', '.pdf', '.md'}

def init_ollama(model_name: str = "qwen3:1.7b"):
    """åˆå§‹åŒ–Ollamaè¿æ¥ï¼Œå¸¦é‡è¯•æœºåˆ¶ï¼Œæ”¯æŒæŒ‡å®šæ¨¡å‹"""
    global llm, current_model
    max_retries = 3
    retry_delay = 2  # ç§’
    
    for attempt in range(max_retries):
        try:
            llm = Ollama(model=model_name)
            # æµ‹è¯•è¿æ¥
            test_response = llm.invoke("ping")
            current_model = model_name
            logger.info(f"æˆåŠŸè¿æ¥åˆ°æœ¬åœ°Ollama {model_name}æ¨¡å‹ï¼Œæµ‹è¯•å“åº”: {test_response[:30]}...")
            return True
        except Exception as e:
            logger.warning(f"è¿æ¥åˆ°Ollama {model_name}æ¨¡å‹å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {str(e)}")
            if attempt < max_retries - 1:
                import time
                time.sleep(retry_delay)
    
    logger.error(f"æ— æ³•åˆå§‹åŒ–Ollama {model_name}æ¨¡å‹è¿æ¥")
    return False

# åˆå§‹åŒ–æ¨¡å‹
if not init_ollama():
    logger.warning("Ollamaæ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼ŒAPIä»å°†å¯åŠ¨ä½†éƒ¨åˆ†åŠŸèƒ½å¯èƒ½æ— æ³•ä½¿ç”¨")

def init_rag(model_name: str = "qwen3:1.7b"):
    """åˆå§‹åŒ–RAGç›¸å…³ç»„ä»¶ï¼Œæ”¯æŒæŒ‡å®šåµŒå…¥æ¨¡å‹"""
    global embeddings
    try:
        embeddings = OllamaEmbeddings(model=model_name)
        logger.info(f"æˆåŠŸåˆå§‹åŒ–OllamaåµŒå…¥æ¨¡å‹: {model_name}")
        return True
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–åµŒå…¥æ¨¡å‹å¤±è´¥: {str(e)}")
        return False

# åˆå§‹åŒ–RAG
if not init_rag():
    logger.warning("RAGç»„ä»¶åˆå§‹åŒ–å¤±è´¥ï¼Œæ–‡æ¡£åŠŸèƒ½å¯èƒ½æ— æ³•ä½¿ç”¨")

def get_knowledge_base_files():
    """è·å–çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯"""
    files_info = {}
    if not KNOWLEDGE_BASE_DIR.exists():
        return files_info
    
    for file_path in KNOWLEDGE_BASE_DIR.rglob("*"):
        if file_path.is_file() and file_path.suffix.lower() in SUPPORTED_EXTENSIONS:
            files_info[str(file_path)] = {
                'mtime': file_path.stat().st_mtime,
                'size': file_path.stat().st_size
            }
    return files_info

def get_vector_store_info():
    """è·å–å‘é‡å­˜å‚¨çš„ä¿¡æ¯"""
    vector_store_path = VECTOR_STORES_DIR / "knowledge_base.faiss"
    metadata_path = VECTOR_STORES_DIR / "knowledge_base_metadata.json"
    
    # æ£€æŸ¥å‘é‡å­˜å‚¨ç›®å½•æ˜¯å¦å­˜åœ¨
    if not vector_store_path.exists() or not vector_store_path.is_dir():
        return None
    
    # æ£€æŸ¥å¿…è¦çš„FAISSæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    index_faiss_path = vector_store_path / "index.faiss"
    index_pkl_path = vector_store_path / "index.pkl"
    
    if not index_faiss_path.exists() or not index_pkl_path.exists():
        return None
    
    try:
        import json
        if metadata_path.exists():
            with open(metadata_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            # å¦‚æœæ²¡æœ‰å…ƒæ•°æ®æ–‡ä»¶ï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
            return {
                'exists': True,
                'mtime': vector_store_path.stat().st_mtime
            }
    except Exception as e:
        logger.warning(f"è¯»å–å‘é‡å­˜å‚¨ä¿¡æ¯å¤±è´¥: {str(e)}")
        return None

def save_vector_store_metadata(files_info):
    """ä¿å­˜å‘é‡å­˜å‚¨çš„å…ƒæ•°æ®"""
    try:
        import json
        metadata_path = VECTOR_STORES_DIR / "knowledge_base_metadata.json"
        metadata = {
            'files_info': files_info,
            'created_at': time.time(),
            'document_count': sum(len(files_info) for files_info in [files_info])
        }
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        logger.info("ä¿å­˜å‘é‡å­˜å‚¨å…ƒæ•°æ®")
    except Exception as e:
        logger.warning(f"ä¿å­˜å‘é‡å­˜å‚¨å…ƒæ•°æ®å¤±è´¥: {str(e)}")

def needs_rebuild():
    """æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°æ„å»ºå‘é‡ç´¢å¼•"""
    # è·å–å½“å‰çŸ¥è¯†åº“æ–‡ä»¶ä¿¡æ¯
    current_files = get_knowledge_base_files()
    if not current_files:
        logger.info("çŸ¥è¯†åº“ç›®å½•ä¸ºç©ºï¼Œæ— éœ€æ„å»ºå‘é‡ç´¢å¼•")
        return False
    
    # è·å–å‘é‡å­˜å‚¨ä¿¡æ¯
    vector_info = get_vector_store_info()
    if not vector_info:
        logger.info("å‘é‡å­˜å‚¨ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°æ„å»º")
        return True
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å…ƒæ•°æ®
    if 'files_info' not in vector_info:
        logger.info("å‘é‡å­˜å‚¨ç¼ºå°‘å…ƒæ•°æ®ï¼Œéœ€è¦é‡æ–°æ„å»º")
        return True
    
    # æ¯”è¾ƒæ–‡ä»¶ä¿¡æ¯
    stored_files = vector_info['files_info']
    
    # æ£€æŸ¥æ–‡ä»¶æ•°é‡æ˜¯å¦å˜åŒ–
    if len(current_files) != len(stored_files):
        logger.info(f"æ–‡ä»¶æ•°é‡å˜åŒ–: å½“å‰{len(current_files)}ä¸ªï¼Œå­˜å‚¨{len(stored_files)}ä¸ª")
        return True
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–
    for file_path, current_info in current_files.items():
        if file_path not in stored_files:
            logger.info(f"æ–°å¢æ–‡ä»¶: {file_path}")
            return True
        
        stored_info = stored_files[file_path]
        if (current_info['mtime'] != stored_info['mtime'] or 
            current_info['size'] != stored_info['size']):
            logger.info(f"æ–‡ä»¶å·²ä¿®æ”¹: {file_path}")
            return True
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶è¢«åˆ é™¤
    for file_path in stored_files:
        if file_path not in current_files:
            logger.info(f"æ–‡ä»¶å·²åˆ é™¤: {file_path}")
            return True
    
    logger.info("çŸ¥è¯†åº“æ–‡ä»¶æ— å˜åŒ–ï¼Œä½¿ç”¨ç°æœ‰å‘é‡ç´¢å¼•")
    return False

def load_saved_vector_stores():
    """åŠ è½½å·²ä¿å­˜çš„å‘é‡ç´¢å¼•"""
    global vector_store
    
    try:
        if not VECTOR_STORES_DIR.exists():
            return False
        
        # ä¼˜å…ˆåŠ è½½å½’ä¸€åŒ–å‘é‡ç´¢å¼•
        normalized_path = VECTOR_STORES_DIR / "knowledge_base_normalized.faiss"
        if normalized_path.exists() and normalized_path.is_dir():
            try:
                # æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                index_faiss_path = normalized_path / "index.faiss"
                index_pkl_path = normalized_path / "index.pkl"
                
                if not index_faiss_path.exists() or not index_pkl_path.exists():
                    logger.warning(f"å½’ä¸€åŒ–å‘é‡ç´¢å¼•æ–‡ä»¶ä¸å®Œæ•´: {normalized_path}")
                    raise Exception("ç´¢å¼•æ–‡ä»¶ä¸å®Œæ•´")
                
                vector_store = FAISS.load_local(str(normalized_path), embeddings)
                logger.info(f"åŠ è½½å½’ä¸€åŒ–å‘é‡ç´¢å¼•: {normalized_path}")
                return True
            except Exception as e:
                logger.warning(f"åŠ è½½å½’ä¸€åŒ–å‘é‡ç´¢å¼•å¤±è´¥ {normalized_path}: {str(e)}")
        
        # å¦‚æœå½’ä¸€åŒ–ç´¢å¼•ä¸å­˜åœ¨ï¼Œå°è¯•åŠ è½½åŸå§‹ç´¢å¼•
        knowledge_base_path = VECTOR_STORES_DIR / "knowledge_base.faiss"
        if knowledge_base_path.exists() and knowledge_base_path.is_dir():
            try:
                vector_store = FAISS.load_local(str(knowledge_base_path), embeddings)
                logger.info(f"åŠ è½½åŸå§‹å‘é‡ç´¢å¼•: {knowledge_base_path}")
                return True
            except Exception as e:
                logger.warning(f"åŠ è½½åŸå§‹å‘é‡ç´¢å¼•å¤±è´¥ {knowledge_base_path}: {str(e)}")
                return False
        
        return False
                    
    except Exception as e:
        logger.error(f"åŠ è½½å‘é‡ç´¢å¼•æ—¶å‡ºé”™: {str(e)}")
        return False

def load_knowledge_base():
    """æ™ºèƒ½åŠ è½½çŸ¥è¯†åº“ - åªåœ¨å¿…è¦æ—¶é‡æ–°å¤„ç†æ–‡æ¡£"""
    try:
        import time
        
        print(f"å¼€å§‹æ™ºèƒ½åŠ è½½çŸ¥è¯†åº“...")
        logger.info("å¼€å§‹æ™ºèƒ½åŠ è½½çŸ¥è¯†åº“...")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°æ„å»º
        if needs_rebuild():
            print("æ£€æµ‹åˆ°çŸ¥è¯†åº“å˜åŒ–ï¼Œé‡æ–°æ„å»ºå‘é‡ç´¢å¼•...")
            logger.info("æ£€æµ‹åˆ°çŸ¥è¯†åº“å˜åŒ–ï¼Œé‡æ–°æ„å»ºå‘é‡ç´¢å¼•...")
            return rebuild_knowledge_base()
        else:
            # å°è¯•åŠ è½½ç°æœ‰å‘é‡ç´¢å¼•
            if load_saved_vector_stores():
                print("æˆåŠŸåŠ è½½ç°æœ‰å‘é‡ç´¢å¼•")
                logger.info("æˆåŠŸåŠ è½½ç°æœ‰å‘é‡ç´¢å¼•")
                return True
            else:
                print("åŠ è½½ç°æœ‰å‘é‡ç´¢å¼•å¤±è´¥ï¼Œé‡æ–°æ„å»º...")
                logger.info("åŠ è½½ç°æœ‰å‘é‡ç´¢å¼•å¤±è´¥ï¼Œé‡æ–°æ„å»º...")
                return rebuild_knowledge_base()
        
    except Exception as e:
        print(f"åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {str(e)}")
        logger.error(f"åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {str(e)}")
        return False

def rebuild_knowledge_base():
    """é‡æ–°æ„å»ºçŸ¥è¯†åº“å‘é‡ç´¢å¼•"""
    try:
        import time
        build_start_time = time.time()
        
        logger.info("ğŸ”¨ å¼€å§‹é‡æ–°æ„å»ºçŸ¥è¯†åº“å‘é‡ç´¢å¼•...")
        print(f"å¼€å§‹æ‰«æçŸ¥è¯†åº“ç›®å½•: {KNOWLEDGE_BASE_DIR}")
        
        if not KNOWLEDGE_BASE_DIR.exists():
            logger.error(f"âŒ çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨: {KNOWLEDGE_BASE_DIR}")
            print(f"çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨: {KNOWLEDGE_BASE_DIR}")
            return False
        
        # æ‰«ææ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
        all_documents = []
        logger.info(f"ğŸ“ æ‰«ææ”¯æŒçš„æ–‡ä»¶ç±»å‹: {SUPPORTED_EXTENSIONS}")
        print(f"æ”¯æŒçš„æ‰©å±•å: {SUPPORTED_EXTENSIONS}")
        
        for file_path in KNOWLEDGE_BASE_DIR.rglob("*"):
            print(f"æ£€æŸ¥æ–‡ä»¶: {file_path}, æ˜¯æ–‡ä»¶: {file_path.is_file()}, æ‰©å±•å: {file_path.suffix.lower()}")
            if file_path.is_file() and file_path.suffix.lower() in SUPPORTED_EXTENSIONS:
                all_documents.append(file_path)
                file_size = file_path.stat().st_size
                logger.info(f"âœ… æ·»åŠ æ–‡æ¡£: {file_path.name} (å¤§å°: {file_size} bytes)")
                print(f"æ·»åŠ æ–‡æ¡£: {file_path}")
        
        logger.info(f"ğŸ“Š æ‰«æå®Œæˆ - æ‰¾åˆ° {len(all_documents)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
        print(f"æ‰¾åˆ° {len(all_documents)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
        
        if not all_documents:
            logger.warning("âš ï¸  çŸ¥è¯†åº“ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶")
            print("çŸ¥è¯†åº“ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶")
            return False
        
        # å¤„ç†æ‰€æœ‰æ–‡æ¡£
        all_splits = []
        processing_start_time = time.time()
        logger.info("ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£å†…å®¹...")
        
        for i, file_path in enumerate(all_documents):
            try:
                file_start_time = time.time()
                logger.info(f"ğŸ“– å¤„ç†æ–‡æ¡£ {i+1}/{len(all_documents)}: {file_path.name}")
                print(f"å¤„ç†æ–‡æ¡£: {file_path}")
                
                # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
                if file_path.suffix.lower() == '.pdf':
                    loader = PyPDFLoader(str(file_path))
                    logger.debug(f"ğŸ”§ ä½¿ç”¨PDFåŠ è½½å™¨: {file_path.name}")
                else:
                    loader = TextLoader(str(file_path), encoding='utf-8')
                    logger.debug(f"ğŸ”§ ä½¿ç”¨æ–‡æœ¬åŠ è½½å™¨: {file_path.name}")
                
                # åŠ è½½æ–‡æ¡£
                documents = loader.load()
                doc_content_length = sum(len(doc.page_content) for doc in documents)
                logger.info(f"ğŸ“‹ æ–‡æ¡£åŠ è½½æˆåŠŸ: {file_path.name} | æ–‡æ¡£æ•°: {len(documents)} | æ€»é•¿åº¦: {doc_content_length}å­—ç¬¦")
                print(f"åŠ è½½æ–‡æ¡£æˆåŠŸ: {file_path}, æ–‡æ¡£æ•°é‡: {len(documents)}")
                
                # æ–‡æœ¬åˆ†å‰²
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000,
                    chunk_overlap=200,
                    length_function=len,
                )
                splits = text_splitter.split_documents(documents)
                all_splits.extend(splits)
                
                file_duration = time.time() - file_start_time
                avg_chunk_size = sum(len(split.page_content) for split in splits) / len(splits) if splits else 0
                logger.info(f"âœ‚ï¸  æ–‡æ¡£åˆ†å‰²å®Œæˆ: {file_path.name} | åˆ†å‰²æ•°: {len(splits)} | å¹³å‡å—å¤§å°: {avg_chunk_size:.0f}å­—ç¬¦ | è€—æ—¶: {file_duration:.3f}ç§’")
                print(f"å¤„ç†æ–‡æ¡£ {file_path.name}: {len(splits)} ä¸ªæ–‡æ¡£å—")
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ–‡æ¡£å¤±è´¥: {file_path} | é”™è¯¯: {str(e)}", exc_info=True)
                print(f"å¤„ç†æ–‡æ¡£ {file_path} å¤±è´¥: {str(e)}")
                continue
        
        processing_duration = time.time() - processing_start_time
        
        if not all_splits:
            logger.error("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡æ¡£")
            print("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡æ¡£")
            return False
        
        total_content_length = sum(len(split.page_content) for split in all_splits)
        avg_chunk_size = total_content_length / len(all_splits)
        logger.info(f"ğŸ“Š æ–‡æ¡£å¤„ç†ç»Ÿè®¡ - æ€»å—æ•°: {len(all_splits)} | æ€»é•¿åº¦: {total_content_length}å­—ç¬¦ | å¹³å‡å—å¤§å°: {avg_chunk_size:.0f}å­—ç¬¦ | å¤„ç†è€—æ—¶: {processing_duration:.3f}ç§’")
        print(f"æ€»å…±å¤„ç†äº† {len(all_splits)} ä¸ªæ–‡æ¡£å—")
        
        # åˆ›å»ºå…¨å±€å‘é‡å­˜å‚¨
        if embeddings is None:
            logger.error("âŒ åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–")
            print("åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–")
            return False
        
        logger.info("ğŸ§® å¼€å§‹åˆ›å»ºå‘é‡å­˜å‚¨...")
        print("å¼€å§‹åˆ›å»ºå‘é‡å­˜å‚¨...")
        
        global vector_store
        vector_start_time = time.time()
        
        # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è€Œä¸æ˜¯L2è·ç¦»
        vector_store = FAISS.from_documents(all_splits, embeddings, distance_strategy="COSINE")
        
        vector_duration = time.time() - vector_start_time
        logger.info(f"ğŸ¯ å‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆ - è€—æ—¶: {vector_duration:.3f}ç§’")
        
        # ä¿å­˜å‘é‡ç´¢å¼•
        save_start_time = time.time()
        vector_store_path = VECTOR_STORES_DIR / "knowledge_base.faiss"
        vector_store.save_local(str(vector_store_path))
        
        # ä¿å­˜å…ƒæ•°æ®
        files_info = get_knowledge_base_files()
        save_vector_store_metadata(files_info)
        
        save_duration = time.time() - save_start_time
        total_duration = time.time() - build_start_time
        
        logger.info(f"ğŸ’¾ å‘é‡ç´¢å¼•ä¿å­˜å®Œæˆ - ä¿å­˜è€—æ—¶: {save_duration:.3f}ç§’")
        logger.info(f"âœ… çŸ¥è¯†åº“æ„å»ºæˆåŠŸ - æ€»è€—æ—¶: {total_duration:.3f}ç§’ | æ–‡æ¡£: {len(all_documents)}ä¸ª | å—æ•°: {len(all_splits)}ä¸ª | å‘é‡ç»´åº¦: {len(embeddings.embed_query('test'))}")
        
        print(f"æˆåŠŸåˆ›å»ºçŸ¥è¯†åº“å‘é‡ç´¢å¼•ï¼ŒåŒ…å« {len(all_splits)} ä¸ªæ–‡æ¡£å—")
        return True
        
    except Exception as e:
        build_duration = time.time() - build_start_time if 'build_start_time' in locals() else 0
        logger.error(f"âŒ çŸ¥è¯†åº“æ„å»ºå¤±è´¥ - è€—æ—¶: {build_duration:.3f}ç§’ | é”™è¯¯: {str(e)}", exc_info=True)
        print(f"é‡æ–°æ„å»ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}")
        return False

# åŠ è½½çŸ¥è¯†åº“
if not load_knowledge_base():
    logger.warning("çŸ¥è¯†åº“åŠ è½½å¤±è´¥ï¼ŒRAGåŠŸèƒ½å¯èƒ½æ— æ³•ä½¿ç”¨")

# å®šä¹‰è¯·æ±‚ä½“æ¨¡å‹
class ChatRequest(BaseModel):
    message: str
    system_prompt: str = "ä½ æ˜¯ä¸€ä¸ª helpful çš„åŠ©æ‰‹ã€‚è¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
    stream: bool = False  # æ˜¯å¦å¯ç”¨æµå¼å“åº”
    session_id: Optional[str] = None  # æ–°å¢ï¼šä¼šè¯ID
    relevance_threshold: float = 0.5  # æé«˜é»˜è®¤é˜ˆå€¼åˆ°0.5
    use_rag: bool = True  # æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨RAGåŠŸèƒ½ï¼Œé»˜è®¤ä¸ºTrue
    max_docs: int = 5  # æ–°å¢ï¼šæœ€å¤§æ£€ç´¢æ–‡æ¡£æ•°é‡
    min_docs: int = 1  # æ–°å¢ï¼šæœ€å°æ£€ç´¢æ–‡æ¡£æ•°é‡

class ModelSwitchRequest(BaseModel):
    model_name: str

# å®šä¹‰å“åº”ä½“æ¨¡å‹
class ChatResponse(BaseModel):
    response: str
    model: str  # ç§»é™¤é»˜è®¤å€¼ï¼ŒåŠ¨æ€è®¾ç½®
    success: bool = True
    error: Optional[str] = None

# è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
@app.get("/api/models")
async def get_available_models():
    """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    return {
        "available_models": available_models,
        "current_model": current_model
    }

# åˆ‡æ¢æ¨¡å‹
@app.post("/api/models/switch")
async def switch_model(request: ModelSwitchRequest):
    """åˆ‡æ¢å½“å‰ä½¿ç”¨çš„æ¨¡å‹"""
    try:
        model_name = request.model_name
        
        if model_name not in available_models:
            raise HTTPException(
                status_code=400, 
                detail=f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}ï¼Œå¯ç”¨æ¨¡å‹: {available_models}"
            )
        
        # åˆå§‹åŒ–æ–°æ¨¡å‹
        if not init_ollama(model_name):
            raise HTTPException(
                status_code=503, 
                detail=f"æ— æ³•åˆå§‹åŒ–æ¨¡å‹: {model_name}"
            )
        
        # åˆå§‹åŒ–RAGç»„ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not init_rag(model_name):
            logger.warning(f"RAGç»„ä»¶åˆå§‹åŒ–å¤±è´¥ï¼Œæ¨¡å‹ {model_name} çš„RAGåŠŸèƒ½å¯èƒ½æ— æ³•ä½¿ç”¨")
        
        logger.info(f"æˆåŠŸåˆ‡æ¢åˆ°æ¨¡å‹: {model_name}")
        return {
            "success": True,
            "message": f"æˆåŠŸåˆ‡æ¢åˆ°æ¨¡å‹: {model_name}",
            "current_model": current_model,
            "previous_model": model_name if model_name != current_model else None
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ‡æ¢æ¨¡å‹å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"åˆ‡æ¢æ¨¡å‹å¤±è´¥: {str(e)}")

# æ–°å»ºä¼šè¯API
@app.post("/api/session/new")
async def new_session():
    session_id = str(uuid.uuid4())
    session_memories[session_id] = ConversationBufferMemory(return_messages=True)
    return {"session_id": session_id}

# è·å–ä¼šè¯å†å²API
@app.get("/api/session/history")
async def get_session_history(session_id: str = Query(...)):
    memory = session_memories.get(session_id)
    if not memory:
        return {"history": []}
    # è¿”å›å†å²æ¶ˆæ¯ï¼ˆuser/aiåˆ†å¼€ï¼‰
    return {"history": [
        {"type": m.type, "content": m.content} for m in memory.chat_memory.messages
    ]}

# è·å–ä¼šè¯æ–‡æ¡£åˆ—è¡¨
@app.get("/api/session/documents")
async def get_session_documents(session_id: str = Query(...)):
    """è·å–ä¼šè¯çš„æ–‡æ¡£åˆ—è¡¨"""
    try:
        session_doc_dir = KNOWLEDGE_BASE_DIR / session_id
        if not session_doc_dir.exists():
            return {"documents": []}
        
        documents = []
        for doc_file in session_doc_dir.iterdir():
            if doc_file.is_file():
                documents.append({
                    "name": doc_file.name,
                    "size": doc_file.stat().st_size,
                    "upload_time": doc_file.stat().st_mtime
                })
        
        return {"documents": documents}
    except Exception as e:
        logger.error(f"è·å–ä¼šè¯æ–‡æ¡£å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}")

# çŸ¥è¯†åº“çŠ¶æ€æ¥å£
@app.get("/api/knowledge-base/status")
async def get_knowledge_base_status():
    """è·å–çŸ¥è¯†åº“çŠ¶æ€"""
    try:
        if not KNOWLEDGE_BASE_DIR.exists():
            return {
                "status": "not_found",
                "message": "çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨",
                "document_count": 0
            }
        
        # ç»Ÿè®¡æ–‡æ¡£æ•°é‡
        document_count = 0
        for file_path in KNOWLEDGE_BASE_DIR.rglob("*"):
            if file_path.is_file() and file_path.suffix.lower() in SUPPORTED_EXTENSIONS:
                document_count += 1
        
        return {
            "status": "loaded" if vector_store is not None else "not_loaded",
            "message": f"çŸ¥è¯†åº“åŒ…å« {document_count} ä¸ªæ–‡æ¡£" if document_count > 0 else "çŸ¥è¯†åº“ç›®å½•ä¸ºç©º",
            "document_count": document_count
        }
    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†åº“çŠ¶æ€å¤±è´¥: {str(e)}")
        return {
            "status": "error",
            "message": f"è·å–çŸ¥è¯†åº“çŠ¶æ€å¤±è´¥: {str(e)}",
            "document_count": 0
        }

# RAGè°ƒè¯•æ¥å£
@app.post("/api/rag/debug")
async def debug_rag_retrieval(
    query: str = Query(..., description="æŸ¥è¯¢æ–‡æœ¬"),
    relevance_threshold: float = Query(0.5, description="ç›¸å…³æ€§é˜ˆå€¼"),
    max_docs: int = Query(5, description="æœ€å¤§æ£€ç´¢æ–‡æ¡£æ•°"),
    test_embedding: bool = Query(False, description="æ˜¯å¦æµ‹è¯•åµŒå…¥æ¨¡å‹")
):
    """è°ƒè¯•RAGæ£€ç´¢è¿‡ç¨‹ï¼Œè¿”å›è¯¦ç»†çš„æ£€ç´¢ä¿¡æ¯"""
    debug_start_time = time.time()
    logger.info(f"ğŸ”¬ RAGè°ƒè¯•è¯·æ±‚å¼€å§‹ - æŸ¥è¯¢: '{query[:50]}...' | é˜ˆå€¼: {relevance_threshold} | æœ€å¤§æ–‡æ¡£æ•°: {max_docs} | æµ‹è¯•åµŒå…¥: {test_embedding}")
    
    try:
        if vector_store is None:
            logger.error("âŒ è°ƒè¯•å¤±è´¥: çŸ¥è¯†åº“æœªåŠ è½½")
            raise HTTPException(status_code=503, detail="çŸ¥è¯†åº“æœªåŠ è½½")
        
        if embeddings is None:
            logger.error("âŒ è°ƒè¯•å¤±è´¥: åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–")
            raise HTTPException(status_code=503, detail="åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–")
        
        # å¦‚æœè¯·æ±‚æµ‹è¯•åµŒå…¥æ¨¡å‹
        if test_embedding:
            logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•åµŒå…¥æ¨¡å‹...")
            try:
                # æµ‹è¯•åµŒå…¥æ¨¡å‹
                embed_start_time = time.time()
                query_embedding = embeddings.embed_query(query)
                embed_duration = time.time() - embed_start_time
                
                query_norm = sum(x*x for x in query_embedding)**0.5
                logger.info(f"ğŸ“Š æŸ¥è¯¢åµŒå…¥å®Œæˆ - è€—æ—¶: {embed_duration:.3f}ç§’ | ç»´åº¦: {len(query_embedding)} | èŒƒæ•°: {query_norm:.4f}")
                
                # æµ‹è¯•æ–‡æ¡£åµŒå…¥
                test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£"
                doc_embed_start_time = time.time()
                doc_embedding = embeddings.embed_documents([test_text])
                doc_embed_duration = time.time() - doc_embed_start_time
                
                doc_norm = sum(x*x for x in doc_embedding[0])**0.5
                logger.info(f"ğŸ“Š æ–‡æ¡£åµŒå…¥å®Œæˆ - è€—æ—¶: {doc_embed_duration:.3f}ç§’ | ç»´åº¦: {len(doc_embedding[0])} | èŒƒæ•°: {doc_norm:.4f}")
                
                total_debug_duration = time.time() - debug_start_time
                logger.info(f"âœ… åµŒå…¥æ¨¡å‹æµ‹è¯•å®Œæˆ - æ€»è€—æ—¶: {total_debug_duration:.3f}ç§’")
                
                return {
                    "embedding_test": {
                        "query_vector_norm": query_norm,
                        "doc_vector_norm": doc_norm,
                        "vector_dimension": len(query_embedding),
                        "query_embed_time": embed_duration,
                        "doc_embed_time": doc_embed_duration
                    }
                }
            except Exception as e:
                logger.error(f"âŒ åµŒå…¥æ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}", exc_info=True)
                return {"embedding_test": {"error": str(e)}}
        
        # ä½¿ç”¨è‡ªé€‚åº”æ£€ç´¢
        logger.info("ğŸ” å¼€å§‹è°ƒè¯•æ£€ç´¢è¿‡ç¨‹...")
        relevant_docs, max_similarity, retrieval_info = adaptive_retrieval(
            vector_store, query, max_docs, 1, relevance_threshold
        )
        
        # è·å–åŸå§‹æ£€ç´¢ç»“æœç”¨äºè°ƒè¯•
        initial_k = min(max_docs * 2, 10)
        logger.debug(f"ğŸ“Š è·å–åŸå§‹æ£€ç´¢ç»“æœ - Kå€¼: {initial_k}")
        
        raw_search_start_time = time.time()
        raw_docs = vector_store.similarity_search_with_score(query, k=initial_k)
        raw_search_duration = time.time() - raw_search_start_time
        
        logger.info(f"âš¡ åŸå§‹æ£€ç´¢å®Œæˆ - è€—æ—¶: {raw_search_duration:.3f}ç§’ | ç»“æœæ•°: {len(raw_docs)}")
        
        # æ„å»ºè°ƒè¯•ä¿¡æ¯
        debug_info = {
            "query": query,
            "retrieval_info": {
                "retrieved": retrieval_info.get("retrieved", 0),
                "relevant": retrieval_info.get("relevant", 0),
                "threshold": float(retrieval_info.get("threshold", 0.0)),
                "max_similarity": float(retrieval_info.get("max_similarity", 0.0)),
                "original_threshold": float(retrieval_info.get("original_threshold", 0.0)),
                "distance_stats": {
                    "min": float(retrieval_info.get("distance_stats", {}).get("min", 0.0)),
                    "max": float(retrieval_info.get("distance_stats", {}).get("max", 0.0)),
                    "avg": float(retrieval_info.get("distance_stats", {}).get("avg", 0.0))
                } if "distance_stats" in retrieval_info else None,
                "is_normalized": retrieval_info.get("is_normalized", False),
                "similarity_method": retrieval_info.get("similarity_method", "unknown"),
                "search_duration": retrieval_info.get("search_duration", 0.0),
                "similarity_duration": retrieval_info.get("similarity_duration", 0.0)
            },
            "raw_docs": []
        }
        
        logger.debug("ğŸ“‹ å¤„ç†åŸå§‹æ£€ç´¢ç»“æœ...")
        for i, (doc, score) in enumerate(raw_docs):
            try:
                # ç¡®ä¿scoreæ˜¯PythonåŸç”Ÿç±»å‹
                score_float = float(score)
                similarity_score = calculate_similarity_score_optimized(score_float, method="sigmoid")
                
                content_preview = doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                logger.debug(f"  æ–‡æ¡£ {i+1}: è·ç¦» {score_float:.4f} -> ç›¸ä¼¼åº¦ {similarity_score:.4f}")
                
                debug_info["raw_docs"].append({
                    "index": i,
                    "distance_score": score_float,
                    "similarity_score": float(similarity_score),
                    "content_preview": content_preview,
                    "metadata": doc.metadata,
                    "content_length": len(doc.page_content)
                })
            except Exception as e:
                logger.warning(f"âš ï¸  å¤„ç†æ–‡æ¡£{i}æ—¶å‡ºé”™: {str(e)}")
                debug_info["raw_docs"].append({
                    "index": i,
                    "distance_score": float(score) if hasattr(score, '__float__') else 0.0,
                    "similarity_score": 0.0,
                    "content_preview": "å¤„ç†å‡ºé”™",
                    "metadata": doc.metadata,
                    "error": str(e)
                })
        
        total_debug_duration = time.time() - debug_start_time
        logger.info(f"âœ… RAGè°ƒè¯•å®Œæˆ - æ€»è€—æ—¶: {total_debug_duration:.3f}ç§’ | æ£€ç´¢åˆ°: {len(raw_docs)}ä¸ª | ç›¸å…³: {retrieval_info.get('relevant', 0)}ä¸ª")
        
        return debug_info
        
    except Exception as e:
        total_debug_duration = time.time() - debug_start_time
        logger.error(f"âŒ RAGè°ƒè¯•å¤±è´¥ - æ€»è€—æ—¶: {total_debug_duration:.3f}ç§’ | é”™è¯¯: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"RAGè°ƒè¯•å¤±è´¥: {str(e)}")

@app.post("/api/chat/rag", response_model=ChatResponse)
async def chat_with_rag(request: ChatRequest):
    """åŸºäºçŸ¥è¯†åº“çš„RAGå¯¹è¯æ¥å£"""
    session_start_time = time.time()
    logger.info(f"ğŸš€ RAGå¯¹è¯è¯·æ±‚å¼€å§‹ - ä¼šè¯ID: {request.session_id} | æ¨¡å‹: {current_model} | ä½¿ç”¨RAG: {request.use_rag}")
    logger.info(f"ğŸ’¬ ç”¨æˆ·é—®é¢˜: '{request.message[:100]}...' (é•¿åº¦: {len(request.message)})")
    
    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åˆå§‹åŒ–
        if llm is None:
            logger.error("âŒ æ¨¡å‹æœªåˆå§‹åŒ–")
            raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡")
        
        # å¦‚æœç”¨æˆ·é€‰æ‹©ä¸ä½¿ç”¨RAGï¼Œåˆ™ç›´æ¥ä½¿ç”¨æ™®é€šå¯¹è¯
        if not request.use_rag:
            logger.info("ğŸ“ ç”¨æˆ·é€‰æ‹©ä¸ä½¿ç”¨RAGï¼Œè½¬ä¸ºæ™®é€šå¯¹è¯æ¨¡å¼")
            # è·å–/åˆ›å»ºmemory
            if not request.session_id:
                raise HTTPException(status_code=400, detail="ç¼ºå°‘session_id")
            memory = session_memories.get(request.session_id)
            if memory is None:
                memory = ConversationBufferMemory(return_messages=True)
                session_memories[request.session_id] = memory
                logger.debug(f"ğŸ†• åˆ›å»ºæ–°çš„ä¼šè¯è®°å¿†: {request.session_id}")
            
            # æ„å»ºå¹²å‡€çš„å†å²è®°å½•ï¼Œé¿å…RAGç›¸å…³æç¤ºè¯æ®‹ç•™
            clean_history = clean_conversation_history(memory)
            
            # æ„å»ºçº¯å‡€çš„æç¤ºè¯
            clean_prompt = f"{request.system_prompt}\n\n{clean_history}ç”¨æˆ·ï¼š{request.message}\nåŠ©æ‰‹ï¼š"
            logger.debug(f"ğŸ“ æ™®é€šå¯¹è¯æç¤ºè¯é•¿åº¦: {len(clean_prompt)}å­—ç¬¦")
            
            # æ‰“å°æœ€ç»ˆæç¤ºè¯
            logger.info("ğŸ” æœ€ç»ˆæäº¤ç»™å¤§æ¨¡å‹çš„æç¤ºè¯:")
            logger.info("="*80)
            logger.info(clean_prompt)
            logger.info("="*80)
            
            # è¿è¡ŒLLMæŸ¥è¯¢
            llm_start_time = time.time()
            result = llm.invoke(clean_prompt)
            llm_duration = time.time() - llm_start_time
            logger.info(f"ğŸ¤– LLMç”Ÿæˆå®Œæˆ - è€—æ—¶: {llm_duration:.3f}ç§’ | å“åº”é•¿åº¦: {len(result)}")
            
            # æ›´æ–°memory
            memory.chat_memory.add_user_message(request.message)
            memory.chat_memory.add_ai_message(result)
            
            total_duration = time.time() - session_start_time
            logger.info(f"âœ… æ™®é€šå¯¹è¯å®Œæˆ - æ€»è€—æ—¶: {total_duration:.3f}ç§’")
            
            return ChatResponse(response=result, model=current_model)
        
        if embeddings is None:
            logger.error("âŒ åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–")
            raise HTTPException(status_code=503, detail="åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œæ— æ³•è¿›è¡ŒRAGå¯¹è¯")
        
        if vector_store is None:
            logger.error("âŒ çŸ¥è¯†åº“æœªåŠ è½½")
            raise HTTPException(status_code=503, detail="çŸ¥è¯†åº“æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥knowledge_baseç›®å½•")
        
        # è·å–/åˆ›å»ºmemory
        if not request.session_id:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘session_id")
        
        memory = session_memories.get(request.session_id)
        if memory is None:
            memory = ConversationBufferMemory(return_messages=True)
            session_memories[request.session_id] = memory
            logger.debug(f"ğŸ†• åˆ›å»ºæ–°çš„ä¼šè¯è®°å¿†: {request.session_id}")
        
        # ç¬¬ä¸€æ­¥ï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆä½¿ç”¨ä¼˜åŒ–åçš„æ£€ç´¢å‡½æ•°ï¼‰
        logger.info(f"ğŸ” å¼€å§‹RAGæ–‡æ¡£æ£€ç´¢ - å‚æ•°: max_docs={request.max_docs}, threshold={request.relevance_threshold}")
        with performance_monitor("RAGæ–‡æ¡£æ£€ç´¢"):
            relevant_docs, max_similarity, retrieval_info = optimized_retrieval(
                vector_store, 
                request.message, 
                max_docs=request.max_docs, 
                relevance_threshold=request.relevance_threshold
            )
        
        # è®°å½•æ£€ç´¢ç»“æœè¯¦ç»†ä¿¡æ¯
        logger.info(f"ğŸ“Š æ£€ç´¢ç»“æœç»Ÿè®¡: {retrieval_info}")
        
        # ç¬¬äºŒæ­¥ï¼šæ ¹æ®æ˜¯å¦æ‰¾åˆ°ç›¸å…³æ–‡æ¡£æ„å»ºä¸åŒçš„æç¤ºè¯
        if relevant_docs:
            # æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼šåŸºäºçŸ¥è¯†åº“å†…å®¹å›ç­”
            context_length = sum(len(doc.page_content) for doc in relevant_docs)
            logger.info(f"ğŸ“š æ„å»ºçŸ¥è¯†åº“ä¸Šä¸‹æ–‡ - æ–‡æ¡£æ•°: {len(relevant_docs)} | æ€»é•¿åº¦: {context_length}å­—ç¬¦")
            
            context = "\n\n".join([doc.page_content for doc in relevant_docs])
            prompt = f"""åŸºäºä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›ç­”é—®é¢˜ï¼š

çŸ¥è¯†åº“å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{request.message}

è¯·åŸºäºçŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¦æ±‚ï¼š
1. ä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“ä¸­æä¾›çš„ä¿¡æ¯
2. å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"çŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯"
3. å›ç­”è¦ç®€æ´å‡†ç¡®ï¼Œä¸è¦é‡å¤çŸ¥è¯†åº“å†…å®¹
4. ä¸è¦æ·»åŠ çŸ¥è¯†åº“ä¸­æ²¡æœ‰çš„ä¿¡æ¯

å›ç­”ï¼š"""
            logger.debug(f"ğŸ“ RAGæç¤ºè¯é•¿åº¦: {len(prompt)}å­—ç¬¦")
        else:
            # æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼šç›´æ¥è®©æ¨¡å‹å›ç­”
            logger.warning("âš ï¸  æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œè½¬ä¸ºé€šç”¨çŸ¥è¯†å›ç­”")
            prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{request.message}

è¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœæ¶‰åŠä¸“ä¸šçŸ¥è¯†ï¼Œè¯·åŸºäºä½ çš„è®­ç»ƒæ•°æ®å›ç­”ã€‚

æ³¨æ„ï¼šçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸ç”¨æˆ·é—®é¢˜ç›´æ¥ç›¸å…³çš„å†…å®¹ï¼Œè¯·åŸºäºä½ çš„é€šç”¨çŸ¥è¯†æ¥å›ç­”ã€‚"""
        
        # æ‰“å°æœ€ç»ˆæç¤ºè¯
        logger.info("ğŸ” æœ€ç»ˆæäº¤ç»™å¤§æ¨¡å‹çš„RAGæç¤ºè¯:")
        logger.info("="*80)
        logger.info(prompt)
        logger.info("="*80)
        
        # è¿è¡ŒLLMæŸ¥è¯¢
        logger.info("ğŸ¤– å¼€å§‹LLMç”Ÿæˆå›ç­”...")
        with performance_monitor("LLMç”Ÿæˆ"):
            llm_start_time = time.time()
            result = llm.invoke(prompt)
            llm_duration = time.time() - llm_start_time
            logger.info(f"ğŸ¤– LLMç”Ÿæˆå®Œæˆ - è€—æ—¶: {llm_duration:.3f}ç§’ | å“åº”é•¿åº¦: {len(result) if result else 0}")
        
        # æ„å»ºå“åº”
        response_text = result if result else "æŠ±æ­‰ï¼Œæœªèƒ½ç”Ÿæˆæœ‰æ•ˆå“åº”ã€‚"
        
        # æ›´æ–°memory
        memory.chat_memory.add_user_message(request.message)
        memory.chat_memory.add_ai_message(response_text)
        
        total_duration = time.time() - session_start_time
        logger.info(f"âœ… RAGå¯¹è¯å®Œæˆ - æ€»è€—æ—¶: {total_duration:.3f}ç§’ | æ£€ç´¢è€—æ—¶: {retrieval_info.get('search_duration', 0):.3f}ç§’ | LLMè€—æ—¶: {llm_duration:.3f}ç§’")
        
        return ChatResponse(response=response_text, model=current_model)
        
    except HTTPException:
        raise
    except Exception as e:
        total_duration = time.time() - session_start_time
        logger.error(f"âŒ RAGå¯¹è¯å¤±è´¥ - æ€»è€—æ—¶: {total_duration:.3f}ç§’ | é”™è¯¯: {str(e)}", exc_info=True)
        return ChatResponse(
            response="",
            model=current_model,
            success=False,
            error=f"RAGå¯¹è¯å¤±è´¥: {str(e)}"
        )

# RAGæµå¼èŠå¤©æ¥å£
@app.post("/api/chat/rag/stream")
async def chat_with_rag_stream(request: ChatRequest):
    """åŸºäºçŸ¥è¯†åº“çš„RAGæµå¼å¯¹è¯æ¥å£"""
    session_start_time = time.time()
    logger.info(f"ğŸš€ RAGæµå¼å¯¹è¯è¯·æ±‚å¼€å§‹ - ä¼šè¯ID: {request.session_id} | æ¨¡å‹: {current_model} | ä½¿ç”¨RAG: {request.use_rag}")
    logger.info(f"ğŸ’¬ ç”¨æˆ·é—®é¢˜: '{request.message[:100]}...' (é•¿åº¦: {len(request.message)})")
    
    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åˆå§‹åŒ–
        if llm is None:
            logger.error("âŒ æ¨¡å‹æœªåˆå§‹åŒ–")
            raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡")
        
        # å¦‚æœç”¨æˆ·é€‰æ‹©ä¸ä½¿ç”¨RAGï¼Œåˆ™ç›´æ¥ä½¿ç”¨æ™®é€šå¯¹è¯
        if not request.use_rag:
            logger.info("ğŸ“ ç”¨æˆ·é€‰æ‹©ä¸ä½¿ç”¨RAGï¼Œè½¬ä¸ºæ™®é€šæµå¼å¯¹è¯æ¨¡å¼")
            # è·å–/åˆ›å»ºmemory
            if not request.session_id:
                raise HTTPException(status_code=400, detail="ç¼ºå°‘session_id")
            memory = session_memories.get(request.session_id)
            if memory is None:
                memory = ConversationBufferMemory(return_messages=True)
                session_memories[request.session_id] = memory
                logger.debug(f"ğŸ†• åˆ›å»ºæ–°çš„ä¼šè¯è®°å¿†: {request.session_id}")
            
            # æ„å»ºå¹²å‡€çš„å†å²è®°å½•ï¼Œé¿å…RAGç›¸å…³æç¤ºè¯æ®‹ç•™
            clean_history = clean_conversation_history(memory)
            
            # æ„å»ºçº¯å‡€çš„æç¤ºè¯
            clean_prompt = f"{request.system_prompt}\n\n{clean_history}ç”¨æˆ·ï¼š{request.message}\nåŠ©æ‰‹ï¼š"
            logger.debug(f"ğŸ“ æ™®é€šæµå¼æç¤ºè¯é•¿åº¦: {len(clean_prompt)}å­—ç¬¦")
            
            # æ‰“å°æœ€ç»ˆæç¤ºè¯
            logger.info("ğŸ” æœ€ç»ˆæäº¤ç»™å¤§æ¨¡å‹çš„æµå¼æç¤ºè¯:")
            logger.info("="*80)
            logger.info(clean_prompt)
            logger.info("="*80)
            
            async def event_generator():
                try:
                    chunk_count = 0
                    empty_chunks = 0
                    full_response = ""
                    stream_start_time = time.time()
                    logger.info("ğŸŒŠ å¼€å§‹æ™®é€šæµå¼ç”Ÿæˆ...")
                    
                    for chunk in llm.stream(clean_prompt):
                        if chunk:
                            chunk_count += 1
                            empty_chunks = 0
                            full_response += chunk
                            yield ServerSentEvent(data=chunk, event="message")
                            await asyncio.sleep(0.01)
                        else:
                            empty_chunks += 1
                            if empty_chunks > 10:
                                logger.warning("âš ï¸  è¿ç»­ç©ºå—è¿‡å¤šï¼Œç»ˆæ­¢æµå¼å“åº”")
                                yield ServerSentEvent(data="", event="error", id="empty_chunks")
                                break
                    
                    stream_duration = time.time() - stream_start_time
                    total_duration = time.time() - session_start_time
                    
                    if chunk_count == 0:
                        logger.error("âŒ æœªæ”¶åˆ°æœ‰æ•ˆæµå¼å“åº”å—")
                        yield ServerSentEvent(data="æœªæ”¶åˆ°æœ‰æ•ˆå“åº”", event="error", id="no_content")
                    else:
                        logger.info(f"âœ… æ™®é€šæµå¼å¯¹è¯å®Œæˆ - æ€»è€—æ—¶: {total_duration:.3f}ç§’ | æµå¼è€—æ—¶: {stream_duration:.3f}ç§’ | å“åº”é•¿åº¦: {len(full_response)} | å—æ•°: {chunk_count}")
                        yield ServerSentEvent(data="", event="end")
                        
                    # æ›´æ–°memory
                    memory.chat_memory.add_user_message(request.message)
                    memory.chat_memory.add_ai_message(full_response)
                        
                except Exception as e:
                    logger.error(f"âŒ æ™®é€šæµå¼å¯¹è¯å‡ºé”™: {str(e)}", exc_info=True)
                    yield ServerSentEvent(data=f"å¤„ç†å‡ºé”™: {str(e)}", event="error")
            
            return EventSourceResponse(event_generator())
        
        if embeddings is None:
            logger.error("âŒ åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–")
            raise HTTPException(status_code=503, detail="åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œæ— æ³•è¿›è¡ŒRAGå¯¹è¯")
        
        if vector_store is None:
            logger.error("âŒ çŸ¥è¯†åº“æœªåŠ è½½")
            raise HTTPException(status_code=503, detail="çŸ¥è¯†åº“æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥knowledge_baseç›®å½•")
        
        # è·å–/åˆ›å»ºmemory
        if not request.session_id:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘session_id")
        
        memory = session_memories.get(request.session_id)
        if memory is None:
            memory = ConversationBufferMemory(return_messages=True)
            session_memories[request.session_id] = memory
            logger.debug(f"ğŸ†• åˆ›å»ºæ–°çš„ä¼šè¯è®°å¿†: {request.session_id}")
        
        # ç¬¬ä¸€æ­¥ï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆä½¿ç”¨ä¼˜åŒ–åçš„æ£€ç´¢å‡½æ•°ï¼‰
        logger.info(f"ğŸ” å¼€å§‹RAGæ–‡æ¡£æ£€ç´¢ - å‚æ•°: max_docs={request.max_docs}, threshold={request.relevance_threshold}")
        relevant_docs, max_similarity, retrieval_info = optimized_retrieval(
            vector_store, 
            request.message, 
            max_docs=request.max_docs, 
            relevance_threshold=request.relevance_threshold
        )
        
        # è®°å½•æ£€ç´¢ç»“æœè¯¦ç»†ä¿¡æ¯
        logger.info(f"ğŸ“Š æ£€ç´¢ç»“æœç»Ÿè®¡: {retrieval_info}")
        
        # ç¬¬äºŒæ­¥ï¼šæ ¹æ®æ˜¯å¦æ‰¾åˆ°ç›¸å…³æ–‡æ¡£æ„å»ºä¸åŒçš„æç¤ºè¯
        if relevant_docs:
            # æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼šåŸºäºçŸ¥è¯†åº“å†…å®¹å›ç­”
            context_length = sum(len(doc.page_content) for doc in relevant_docs)
            logger.info(f"ğŸ“š æ„å»ºçŸ¥è¯†åº“ä¸Šä¸‹æ–‡ - æ–‡æ¡£æ•°: {len(relevant_docs)} | æ€»é•¿åº¦: {context_length}å­—ç¬¦")
            
            context = "\n\n".join([doc.page_content for doc in relevant_docs])
            prompt = f"""åŸºäºä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›ç­”é—®é¢˜ï¼š

çŸ¥è¯†åº“å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{request.message}

è¯·åŸºäºçŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¦æ±‚ï¼š
1. ä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“ä¸­æä¾›çš„ä¿¡æ¯
2. å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"çŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯"
3. å›ç­”è¦ç®€æ´å‡†ç¡®ï¼Œä¸è¦é‡å¤çŸ¥è¯†åº“å†…å®¹
4. ä¸è¦æ·»åŠ çŸ¥è¯†åº“ä¸­æ²¡æœ‰çš„ä¿¡æ¯

å›ç­”ï¼š"""
            logger.debug(f"ğŸ“ RAGæç¤ºè¯é•¿åº¦: {len(prompt)}å­—ç¬¦")
        else:
            # æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼šç›´æ¥è®©æ¨¡å‹å›ç­”
            logger.warning("âš ï¸  æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œè½¬ä¸ºé€šç”¨çŸ¥è¯†å›ç­”")
            prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{request.message}

è¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœæ¶‰åŠä¸“ä¸šçŸ¥è¯†ï¼Œè¯·åŸºäºä½ çš„è®­ç»ƒæ•°æ®å›ç­”ã€‚

æ³¨æ„ï¼šçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸ç”¨æˆ·é—®é¢˜ç›´æ¥ç›¸å…³çš„å†…å®¹ï¼Œè¯·åŸºäºä½ çš„é€šç”¨çŸ¥è¯†æ¥å›ç­”ã€‚"""
        
        # æ‰“å°æœ€ç»ˆæç¤ºè¯
        logger.info("ğŸ” æœ€ç»ˆæäº¤ç»™å¤§æ¨¡å‹çš„RAGæµå¼æç¤ºè¯:")
        logger.info("="*80)
        logger.info(prompt)
        logger.info("="*80)
        
        async def event_generator():
            try:
                chunk_count = 0
                empty_chunks = 0
                full_response = ""
                stream_start_time = time.time()
                logger.info("ğŸŒŠ å¼€å§‹RAGæµå¼ç”Ÿæˆ...")
                
                for chunk in llm.stream(prompt):
                    if chunk:
                        chunk_count += 1
                        empty_chunks = 0  # é‡ç½®ç©ºå—è®¡æ•°å™¨
                        full_response += chunk
                        yield ServerSentEvent(data=chunk, event="message")
                        await asyncio.sleep(0.01)  # çŸ­æš‚å»¶è¿Ÿï¼Œé˜²æ­¢å‰ç«¯å¤„ç†ä¸è¿‡æ¥
                    else:
                        empty_chunks += 1
                        logger.warning(f"æ”¶åˆ°ç©ºçš„æµå¼å“åº”å— ({empty_chunks})")
                        if empty_chunks > 10:  # è¿ç»­10ä¸ªç©ºå—åˆ™ç»ˆæ­¢
                            logger.error("è¿ç»­æ”¶åˆ°å¤šä¸ªç©ºå—ï¼Œç»ˆæ­¢æµå¼å“åº”")
                            yield ServerSentEvent(data="", event="error", id="empty_chunks")
                            break
                
                stream_duration = time.time() - stream_start_time
                total_duration = time.time() - session_start_time
                
                if chunk_count == 0:
                    logger.error("âŒ æœªæ”¶åˆ°æœ‰æ•ˆæµå¼å“åº”å—")
                    yield ServerSentEvent(data="æœªæ”¶åˆ°æœ‰æ•ˆå“åº”", event="error", id="no_content")
                else:
                    logger.info(f"âœ… RAGæµå¼å¯¹è¯å®Œæˆ - æ€»è€—æ—¶: {total_duration:.3f}ç§’ | æ£€ç´¢è€—æ—¶: {retrieval_info.get('search_duration', 0):.3f}ç§’ | æµå¼è€—æ—¶: {stream_duration:.3f}ç§’ | å“åº”é•¿åº¦: {len(full_response)} | å—æ•°: {chunk_count}")
                    yield ServerSentEvent(data="", event="end")
                    
                # æ›´æ–°memory
                memory.chat_memory.add_user_message(request.message)
                memory.chat_memory.add_ai_message(full_response)
                    
            except Exception as e:
                logger.error(f"âŒ RAGæµå¼å¤„ç†å‡ºé”™: {str(e)}", exc_info=True)
                yield ServerSentEvent(data=f"å¤„ç†å‡ºé”™: {str(e)}", event="error")
        
        return EventSourceResponse(event_generator())
        
    except HTTPException:
        raise
    except Exception as e:
        total_duration = time.time() - session_start_time
        logger.error(f"âŒ RAGæµå¼å¯¹è¯å¤±è´¥ - æ€»è€—æ—¶: {total_duration:.3f}ç§’ | é”™è¯¯: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"RAGæµå¼å¯¹è¯å¤±è´¥: {str(e)}")

@app.post("/api/chat", response_model=ChatResponse, description="ä¸Llama3.2æ¨¡å‹è¿›è¡Œå¯¹è¯")
async def chat(request: ChatRequest):
    """æ™®é€šå¯¹è¯æ¥å£ï¼Œä¸€æ¬¡æ€§è¿”å›å®Œæ•´å“åº”"""
    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åˆå§‹åŒ–
        if llm is None:
            raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡")
        
        if request.stream:
            # å¦‚æœè¯·æ±‚æµå¼å“åº”ï¼Œå¼•å¯¼åˆ°æµå¼æ¥å£
            raise HTTPException(status_code=400, detail="è¯·ä½¿ç”¨/api/chat/streamæ¥å£è·å–æµå¼å“åº”")
            
        logger.info(f"æ”¶åˆ°ç”¨æˆ·æ¶ˆæ¯: {request.message[:100]}...")  # é™åˆ¶æ—¥å¿—é•¿åº¦
        
        # è·å–/åˆ›å»ºmemory
        if not request.session_id:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘session_id")
        memory = session_memories.get(request.session_id)
        if memory is None:
            memory = ConversationBufferMemory(return_messages=True)
            session_memories[request.session_id] = memory
        
        # æ„å»ºå¹²å‡€çš„å†å²è®°å½•ï¼Œé¿å…RAGç›¸å…³æç¤ºè¯æ®‹ç•™
        clean_history = clean_conversation_history(memory)
        
        # æ„å»ºçº¯å‡€çš„æç¤ºè¯
        clean_prompt = f"{request.system_prompt}\n\n{clean_history}ç”¨æˆ·ï¼š{request.message}\nåŠ©æ‰‹ï¼š"
        logger.debug(f"ğŸ“ æ™®é€šå¯¹è¯æç¤ºè¯é•¿åº¦: {len(clean_prompt)}å­—ç¬¦")
        
        # æ‰“å°æœ€ç»ˆæç¤ºè¯
        logger.info("ğŸ” æœ€ç»ˆæäº¤ç»™å¤§æ¨¡å‹çš„æ™®é€šå¯¹è¯æç¤ºè¯:")
        logger.info("="*80)
        logger.info(clean_prompt)
        logger.info("="*80)
        
        # è¿è¡ŒLLMæŸ¥è¯¢
        result = llm.invoke(clean_prompt)
        
        # éªŒè¯ç»“æœ
        if not result or not result.strip():
            logger.warning("æ¨¡å‹è¿”å›ç©ºå“åº”")
            return ChatResponse(
                response="æŠ±æ­‰ï¼Œæœªèƒ½è·å–åˆ°æœ‰æ•ˆå“åº”ã€‚",
                model=current_model,
                success=False,
                error="æ¨¡å‹è¿”å›ç©ºå“åº”"
            )
        
        logger.info(f"æ¨¡å‹å“åº”: {result[:100]}...")  # åªæ‰“å°å‰100ä¸ªå­—ç¬¦
        # æ›´æ–°memory
        memory.chat_memory.add_user_message(request.message)
        memory.chat_memory.add_ai_message(result)
        return ChatResponse(response=result, model=current_model)
    
    except HTTPException:
        # é‡æ–°æŠ›å‡ºFastAPI HTTPå¼‚å¸¸
        raise
    except Exception as e:
        # æ•è·æ‰€æœ‰å…¶ä»–å¼‚å¸¸å¹¶è®°å½•
        logger.error(f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}", exc_info=True)
        # è¿”å›ç»“æ„åŒ–é”™è¯¯å“åº”
        return ChatResponse(
            response="",
            model=current_model,
            success=False,
            error=f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}"
        )

@app.post("/api/chat/stream", description="ä¸Llama3.2æ¨¡å‹è¿›è¡Œå¯¹è¯ï¼ˆæµå¼å“åº”ï¼‰")
async def chat_stream(request: ChatRequest):
    """æµå¼å¯¹è¯æ¥å£ï¼Œé€æ®µè¿”å›å“åº”å†…å®¹"""
    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åˆå§‹åŒ–
        if llm is None:
            raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡")
        
        logger.info(f"æ”¶åˆ°ç”¨æˆ·æµå¼æ¶ˆæ¯: {request.message[:100]}...")
        
        # è·å–/åˆ›å»ºmemory
        if not request.session_id:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘session_id")
        memory = session_memories.get(request.session_id)
        if memory is None:
            memory = ConversationBufferMemory(return_messages=True)
            session_memories[request.session_id] = memory
        
        # æ„å»ºå¹²å‡€çš„å†å²è®°å½•ï¼Œé¿å…RAGç›¸å…³æç¤ºè¯æ®‹ç•™
        history = clean_conversation_history(memory)
        
        # åˆ›å»ºæç¤ºæ¨¡æ¿
        prompt = f"{request.system_prompt}\n\n"
        # ConversationChainè‡ªåŠ¨æ³¨å…¥å†å²
        full_prompt = prompt + "{history}\nç”¨æˆ·é—®ï¼š{input}\nåŠ©æ‰‹ç­”ï¼š"
        prompt_filled = full_prompt.replace("{history}", history).replace("{input}", request.message)
        
        # æ‰“å°æœ€ç»ˆæç¤ºè¯
        logger.info("ğŸ” æœ€ç»ˆæäº¤ç»™å¤§æ¨¡å‹çš„æ™®é€šæµå¼æç¤ºè¯:")
        logger.info("="*80)
        logger.info(prompt_filled)
        logger.info("="*80)
        
        async def event_generator():
            try:
                # ä½¿ç”¨Ollamaçš„æµå¼ç”Ÿæˆ
                chunk_count = 0
                empty_chunks = 0
                full_response = ""
                
                for chunk in llm.stream(prompt_filled):
                    if chunk:
                        chunk_count += 1
                        empty_chunks = 0  # é‡ç½®ç©ºå—è®¡æ•°å™¨
                        full_response += chunk
                        yield ServerSentEvent(data=chunk, event="message")
                        await asyncio.sleep(0.01)  # çŸ­æš‚å»¶è¿Ÿï¼Œé˜²æ­¢å‰ç«¯å¤„ç†ä¸è¿‡æ¥
                    else:
                        empty_chunks += 1
                        logger.warning(f"æ”¶åˆ°ç©ºçš„æµå¼å“åº”å— ({empty_chunks})")
                        if empty_chunks > 10:  # è¿ç»­10ä¸ªç©ºå—åˆ™ç»ˆæ­¢
                            logger.error("è¿ç»­æ”¶åˆ°å¤šä¸ªç©ºå—ï¼Œç»ˆæ­¢æµå¼å“åº”")
                            yield ServerSentEvent(data="", event="error", id="empty_chunks")
                            break
                
                if chunk_count == 0:
                    logger.warning("æœªæ”¶åˆ°ä»»ä½•æœ‰æ•ˆæµå¼å“åº”å—")
                    yield ServerSentEvent(data="æœªæ”¶åˆ°æœ‰æ•ˆå“åº”", event="error", id="no_content")
                else:
                    # å‘é€ç»“æŸä¿¡å·
                    yield ServerSentEvent(data="", event="end")
                    
                # æ›´æ–°memory
                memory.chat_memory.add_user_message(request.message)
                memory.chat_memory.add_ai_message(full_response)
                    
            except Exception as e:
                logger.error(f"å¤„ç†æµå¼è¯·æ±‚æ—¶å‡ºé”™: {str(e)}", exc_info=True)
                yield ServerSentEvent(data=f"å¤„ç†å‡ºé”™: {str(e)}", event="error")
        
        return EventSourceResponse(event_generator())
    
    except HTTPException:
        # é‡æ–°æŠ›å‡ºFastAPI HTTPå¼‚å¸¸
        raise
    except Exception as e:
        logger.error(f"å¤„ç†æµå¼è¯·æ±‚æ—¶å‡ºé”™: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å¤„ç†æµå¼è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")

# æ€§èƒ½åˆ†æAPI
@app.post("/api/performance/analyze")
async def analyze_rag_performance(
    request: dict
):
    """åˆ†æRAGæ€§èƒ½ç“¶é¢ˆ"""
    try:
        # ä»è¯·æ±‚ä½“ä¸­æå–å‚æ•°
        query = request.get("query", "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ")
        max_docs = request.get("max_docs", 5)
        relevance_threshold = request.get("relevance_threshold", 0.5)
        
        if vector_store is None:
            raise HTTPException(status_code=503, detail="çŸ¥è¯†åº“æœªåŠ è½½")
        
        if embeddings is None:
            raise HTTPException(status_code=503, detail="åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–")
        
        performance_data = {}
        
        # æµ‹è¯•åµŒå…¥æ¨¡å‹æ€§èƒ½
        with performance_monitor("åµŒå…¥æ¨¡å‹æµ‹è¯•"):
            try:
                query_embedding = embeddings.embed_query(query)
                performance_data["embedding_time"] = "æ­£å¸¸"
                performance_data["embedding_dimension"] = len(query_embedding)
            except Exception as e:
                performance_data["embedding_time"] = f"å¤±è´¥: {str(e)}"
        
        # æµ‹è¯•å‘é‡æ£€ç´¢æ€§èƒ½
        with performance_monitor("å‘é‡æ£€ç´¢æµ‹è¯•"):
            try:
                docs = vector_store.similarity_search_with_score(query, k=max_docs)
                performance_data["retrieval_time"] = "æ­£å¸¸"
                performance_data["retrieved_docs"] = len(docs)
            except Exception as e:
                performance_data["retrieval_time"] = f"å¤±è´¥: {str(e)}"
        
        # æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—æ€§èƒ½
        if "retrieved_docs" in performance_data and performance_data["retrieved_docs"] > 0:
            with performance_monitor("ç›¸ä¼¼åº¦è®¡ç®—æµ‹è¯•"):
                try:
                    distances = []
                    for result in docs:
                        if isinstance(result, tuple) and len(result) == 2:
                            distances.append(float(result[1]))
                    
                    if distances:
                        # æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—
                        for distance in distances[:3]:  # åªæµ‹è¯•å‰3ä¸ª
                            calculate_similarity_score_optimized(distance)
                        performance_data["similarity_calculation"] = "æ­£å¸¸"
                    else:
                        performance_data["similarity_calculation"] = "æ— æœ‰æ•ˆè·ç¦»åˆ†æ•°"
                except Exception as e:
                    performance_data["similarity_calculation"] = f"å¤±è´¥: {str(e)}"
        
        # æµ‹è¯•LLMæ€§èƒ½
        with performance_monitor("LLMæµ‹è¯•"):
            try:
                test_prompt = f"è¯·ç®€å•å›ç­”ï¼š{query}"
                result = llm.invoke(test_prompt)
                performance_data["llm_time"] = "æ­£å¸¸"
                performance_data["llm_response_length"] = len(result) if result else 0
            except Exception as e:
                performance_data["llm_time"] = f"å¤±è´¥: {str(e)}"
        
        # å®Œæ•´RAGæµç¨‹æµ‹è¯•
        with performance_monitor("å®Œæ•´RAGæµç¨‹"):
            try:
                relevant_docs, max_similarity, retrieval_info = optimized_retrieval(
                    vector_store, query, max_docs, relevance_threshold
                )
                performance_data["full_rag_time"] = "æ­£å¸¸"
                performance_data["relevant_docs_count"] = len(relevant_docs)
                performance_data["max_similarity"] = max_similarity
            except Exception as e:
                performance_data["full_rag_time"] = f"å¤±è´¥: {str(e)}"
        
        return {
            "status": "success",
            "performance_data": performance_data,
            "recommendations": generate_performance_recommendations(performance_data)
        }
        
    except Exception as e:
        logger.error(f"æ€§èƒ½åˆ†æå¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æ€§èƒ½åˆ†æå¤±è´¥: {str(e)}")

def generate_performance_recommendations(performance_data: dict) -> list:
    """æ ¹æ®æ€§èƒ½æ•°æ®ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
    recommendations = []
    
    # æ£€æŸ¥åµŒå…¥æ¨¡å‹æ€§èƒ½
    if "embedding_time" in performance_data and "å¤±è´¥" in performance_data["embedding_time"]:
        recommendations.append("åµŒå…¥æ¨¡å‹å“åº”æ…¢ï¼Œå»ºè®®æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€")
    
    # æ£€æŸ¥å‘é‡æ£€ç´¢æ€§èƒ½
    if "retrieval_time" in performance_data and "å¤±è´¥" in performance_data["retrieval_time"]:
        recommendations.append("å‘é‡æ£€ç´¢å¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥FAISSç´¢å¼•çŠ¶æ€")
    
    # æ£€æŸ¥LLMæ€§èƒ½
    if "llm_time" in performance_data and "å¤±è´¥" in performance_data["llm_time"]:
        recommendations.append("LLMå“åº”æ…¢ï¼Œå»ºè®®æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€")
    
    # æ£€æŸ¥å®Œæ•´æµç¨‹æ€§èƒ½
    if "full_rag_time" in performance_data and "å¤±è´¥" in performance_data["full_rag_time"]:
        recommendations.append("å®Œæ•´RAGæµç¨‹å¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥å„ç»„ä»¶çŠ¶æ€")
    
    # å¦‚æœæ²¡æœ‰æ˜æ˜¾é—®é¢˜ï¼Œæä¾›ä¸€èˆ¬æ€§å»ºè®®
    if not recommendations:
        recommendations.extend([
            "æ€§èƒ½è¡¨ç°æ­£å¸¸",
            "å¦‚éœ€è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå¯è€ƒè™‘ï¼š",
            "- å‡å°‘æ£€ç´¢æ–‡æ¡£æ•°é‡",
            "- è°ƒæ•´ç›¸å…³æ€§é˜ˆå€¼",
            "- ä½¿ç”¨æ›´å¿«çš„åµŒå…¥æ¨¡å‹",
            "- ä¼˜åŒ–å‘é‡ç´¢å¼•"
        ])
    
    return recommendations

@app.get("/api/health", description="æ£€æŸ¥APIå’Œæ¨¡å‹å¥åº·çŠ¶æ€")
async def health_check():
    """æ£€æŸ¥APIå’Œæ¨¡å‹æ˜¯å¦æ­£å¸¸è¿è¡Œ"""
    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åˆå§‹åŒ–
        if llm is None:
            return {
                "status": "unhealthy",
                "model": current_model,
                "available_models": available_models,
                "message": "æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡",
                "test_response": None,
                "working_directory": str(WORKING_DIR.absolute()),
                "knowledge_base_dir": str(KNOWLEDGE_BASE_DIR.absolute()),
                "vector_stores_dir": str(VECTOR_STORES_DIR.absolute())
            }
        
        # ç®€å•æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ç”¨
        test_response = llm.invoke("ä½ å¥½")
        return {
            "status": "healthy",
            "model": current_model,
            "available_models": available_models,
            "message": f"APIå’Œæ¨¡å‹({current_model})å‡æ­£å¸¸è¿è¡Œ",
            "test_response": test_response[:50],  # è¿”å›éƒ¨åˆ†æµ‹è¯•å“åº”
            "working_directory": str(WORKING_DIR.absolute()),
            "knowledge_base_dir": str(KNOWLEDGE_BASE_DIR.absolute()),
            "vector_stores_dir": str(VECTOR_STORES_DIR.absolute())
        }
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "status": "unhealthy",
            "model": current_model,
            "available_models": available_models,
            "message": f"æ¨¡å‹ä¸å¯ç”¨: {str(e)}",
            "test_response": None,
            "working_directory": str(WORKING_DIR.absolute()),
            "knowledge_base_dir": str(KNOWLEDGE_BASE_DIR.absolute()),
            "vector_stores_dir": str(VECTOR_STORES_DIR.absolute())
        }

# æ·»åŠ å†å²è®°å½•æ¸…ç†å‡½æ•°
def clean_conversation_history(memory: ConversationBufferMemory) -> str:
    """
    æ¸…ç†å¯¹è¯å†å²ï¼Œç§»é™¤RAGç›¸å…³çš„æç¤ºè¯å’Œå†…å®¹
    """
    clean_history = ""
    rag_keywords = [
        "åŸºäºä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹", "æ ¹æ®æ–‡æ¡£å†…å®¹", "çŸ¥è¯†åº“ä¸­", "åŸºäºçŸ¥è¯†åº“", "æ–‡æ¡£ä¸­æåˆ°",
        "è¯·åŸºäºçŸ¥è¯†åº“å†…å®¹å›ç­”", "çŸ¥è¯†åº“å†…å®¹ï¼š", "æ ¹æ®æä¾›çš„æ–‡æ¡£", "æ–‡æ¡£æ˜¾ç¤º",
        "åŸºäºä¸Šè¿°æ–‡æ¡£", "æ–‡æ¡£èµ„æ–™æ˜¾ç¤º", "æ ¹æ®ç›¸å…³æ–‡æ¡£", "æ–‡æ¡£ä¿¡æ¯è¡¨æ˜"
    ]
    
    for m in memory.chat_memory.messages:
        if m.type == "human":
            clean_history += f"ç”¨æˆ·ï¼š{m.content}\n"
        elif m.type == "ai":
            ai_content = m.content
            # æ£€æŸ¥æ˜¯å¦åŒ…å«RAGç›¸å…³å…³é”®è¯
            is_rag_response = any(keyword in ai_content for keyword in rag_keywords)
            
            # å¦‚æœä¸æ˜¯RAGå“åº”ï¼Œæˆ–è€…æ˜¯ç»è¿‡æ¸…ç†çš„å“åº”ï¼Œåˆ™ä¿ç•™
            if not is_rag_response:
                clean_history += f"åŠ©æ‰‹ï¼š{ai_content}\n"
            else:
                logger.debug(f"è¿‡æ»¤RAGå“åº”: {ai_content[:50]}...")
    
    return clean_history

if __name__ == "__main__":
    import uvicorn
    logger.info("å¯åŠ¨APIæœåŠ¡å™¨...")
    uvicorn.run(app, host="0.0.0.0", port=8000)
